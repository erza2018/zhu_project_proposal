## Ethicality in Personal Assistant Response Accuracy

# The Project

In the past decade, personal assistants have become more and more prevalent in our daily lives. Assistants such as Siri, Cortana, and Google Assistant have become intimately connected to our day to day by embedding themselves in our phones, laptops, homes, and cars. The training of these personal assistants uses deep neural networks and machine learning to parse user language and generate understandable responses. For this project, I want to conduct an ethical exploration on whether personal assistants are biased towards certain demographics. Are these assistants more or less accurate at understanding specific subsets of the population based on the gender or dialect of the speaker?

# Goals

- Learn about how personal assistants such as Siri and Alexa are trained
- Explore the ethics of these personal assistants' training process
- Conduct our own ethical and efficient data collection
- Learn about how companies creating personal assistants can attempt to counteract biases in their neural networks


# Lingering Questions
- How will we get data? Will we collect our own or will we use existing data sets. If the goal of the research is to explore whether demographic factorrs affect response accuracy, what information do existing data sets include about their research participants?
- How will accuracy be defined?
- How is each personal assistant trained? What are the similarities and differences? Is one method/assistant more susceptible to bias because of it?
- If there is an ethical bias encountered, how can companies creating personal assistants try and counteract the bias?
- What are the real world implications of ethical biases in the realm of personal assistants?



